{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c988869a",
   "metadata": {},
   "source": [
    "# Uncovering Healthcare Inefficiencies - Model Building and Evaluation\n",
    "\n",
    "This notebook focuses on building, training, and evaluating various models to determine the best performing model for our dataset.\n",
    "\n",
    "The models included in this notebook are:\n",
    "\n",
    "1. **Logistic Regression**: Used as the baseline model.\n",
    "2. **Recurrent Neural Network (RNN)**: For capturing temporal dependencies.\n",
    "3. **Convolutional Neural Network (CNN)**: For capturing spatial hierarchies.\n",
    "4. **DBSCAN**: Unsupervised clustering to identify clusters and noise.\n",
    "\n",
    "Each model undergoes the following steps:\n",
    "\n",
    "1. **Data Preprocessing**: Standardizing and preparing data.\n",
    "2. **Model Building**: Constructing model architecture.\n",
    "3. **Model Training**: Training the model.\n",
    "4. **Model Evaluation**: Assessing performance.\n",
    "5. **Results Analysis**: Comparing results to determine the best model.\n",
    "\n",
    "\n",
    "The objective is to identify the model that yields the best results in terms of accuracy and other relevant metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832db883-2ab1-42e5-ae63-18a0d0753abb",
   "metadata": {},
   "source": [
    "## Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f34cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import PowerTransformer, RobustScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Embedding, SimpleRNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # supress warning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3cd24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/samantharivas/Documents/UNIVERSITY OF SAN DIEGO/ADS599/healthcare-market-saturation-fraud\n"
     ]
    }
   ],
   "source": [
    "# Check current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd07646",
   "metadata": {},
   "source": [
    "## Import data from preprocessing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf93427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "# all the training/validation/test dataframes\n",
    "x_train = pd.read_csv('data/x_train.csv') \n",
    "x_train_scaled = pd.read_csv('data/x_train_scaled.csv')\n",
    "x_train_pca = pd.read_csv('data/x_train_pca.csv')\n",
    "x_train_scaled_pca = pd.read_csv('data/x_train_scaled_pca.csv')\n",
    "\n",
    "x_val = pd.read_csv('data/x_val.csv') \n",
    "x_val_scaled = pd.read_csv('data/x_val_scaled.csv')\n",
    "x_val_pca = pd.read_csv('data/x_val_pca.csv')\n",
    "x_val_scaled_pca = pd.read_csv('data/x_val_scaled_pca.csv')\n",
    "\n",
    "x_test = pd.read_csv('data/x_test.csv')\n",
    "x_test_scaled = pd.read_csv('data/x_test_scaled.csv')\n",
    "x_test_pca = pd.read_csv('data/x_test_pca.csv')\n",
    "x_test_scaled_pca = pd.read_csv('data/x_test_scaled_pca.csv')\n",
    "\n",
    "\n",
    "# all the labels\n",
    "y_train = np.ravel(pd.read_csv('data/y_train.csv'))\n",
    "y_val = np.ravel(pd.read_csv('data/y_val.csv'))\n",
    "y_test = np.ravel(pd.read_csv('data/y_test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d9c92",
   "metadata": {},
   "source": [
    "## DataTransformation \n",
    "\n",
    "### Yeo Johnson transformation of data\n",
    "\n",
    "We wanted to add in additional dataframes to see if there was a difference in modeling performance. This Yeo-Johnson transformation was one of them, another would be to do transformation + scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd538566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed data\n",
    "# create copy of df \n",
    "x_train_transformed = x_train.copy()\n",
    "x_val_transformed = x_val.copy()\n",
    "x_test_transformed = x_test.copy()\n",
    "\n",
    "# get numeric columns\n",
    "numeric_columns = x_train_transformed.select_dtypes(include=['float']).columns\n",
    "\n",
    "def yeo_johnson_transform(column):\n",
    "    # Create an instance of PowerTransformer with Yeo-Johnson method\n",
    "    pt = PowerTransformer(method='yeo-johnson')\n",
    "    \n",
    "    # Reshape column for PowerTransformer which expects 2D input\n",
    "    column_reshaped = column.values.reshape(-1, 1)\n",
    "    \n",
    "    # Fit and transform the column\n",
    "    transformed_col = pt.fit_transform(column_reshaped)\n",
    "    \n",
    "    # Flatten the result to match original column shape\n",
    "    return transformed_col.flatten()\n",
    "\n",
    "# Apply Box-Cox transformation to each numeric column\n",
    "for col in numeric_columns:\n",
    "    x_train_transformed[col] = yeo_johnson_transform(x_train_transformed[col])\n",
    "    x_val_transformed[col] = yeo_johnson_transform(x_val_transformed[col])\n",
    "    x_test_transformed[col] = yeo_johnson_transform(x_test_transformed[col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e87ee19",
   "metadata": {},
   "source": [
    "### Log transformed + scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87097630",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_trans_scaled = x_train_transformed.copy()\n",
    "x_val_trans_scaled = x_val_transformed.copy()\n",
    "x_test_trans_scaled = x_test_transformed.copy()\n",
    "\n",
    "scaler = RobustScaler()\n",
    "x_train_trans_scaled[numeric_columns] = scaler.fit_transform(x_train_trans_scaled[numeric_columns])\n",
    "x_val_trans_scaled[numeric_columns] = scaler.transform(x_val_trans_scaled[numeric_columns])\n",
    "x_test_trans_scaled[numeric_columns] = scaler.transform(x_test_trans_scaled[numeric_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced5f2b",
   "metadata": {},
   "source": [
    "## Baseline Model Selection - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4461c82d",
   "metadata": {},
   "source": [
    "We'll first start by deciding on a baseline model for comparison against other models. The confusion matrix will be used to determine which dataframe will be ingested for each machine learning model. We currently have the following dataframes/data to feed into the logistic regression model:\n",
    "\n",
    "* The preprocessed data - x_train\n",
    "* The transformed data - x_train_tranformed\n",
    "* The scaled data - x_train_scaled\n",
    "* The transformed + scaled data - x_train_trans_scaled\n",
    "* The pca transformed data - x_train_pca\n",
    "* The scaled data + pca - x_train_scaled_pca\n",
    "\n",
    "Based on the results of the baseline regression model, we can choose a dataframe to carry through the modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2985f35",
   "metadata": {},
   "source": [
    "### Create and train Logistic Regression Model for unscaled data\n",
    "\n",
    "This is the first model with the data that has been preprocessed but not scaled nor transformed for normality. The accuracy was terrible, the precision and F-score were non existant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18a08832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samantharivas/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6181815860532515\n",
      "Validation Confusion Matrix:\n",
      "[[59423 57408]\n",
      " [ 2405 37417]]\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.51      0.67    116831\n",
      "           1       0.39      0.94      0.56     39822\n",
      "\n",
      "    accuracy                           0.62    156653\n",
      "   macro avg       0.68      0.72      0.61    156653\n",
      "weighted avg       0.82      0.62      0.64    156653\n",
      "\n",
      "Test Accuracy: 0.6179414505853664\n",
      "Test Confusion Matrix:\n",
      "[[59410 57422]\n",
      " [ 2429 37393]]\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.51      0.67    116832\n",
      "           1       0.39      0.94      0.56     39822\n",
      "\n",
      "    accuracy                           0.62    156654\n",
      "   macro avg       0.68      0.72      0.61    156654\n",
      "weighted avg       0.82      0.62      0.64    156654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logreg model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_val_pred = model.predict(x_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_confusion_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n",
    "print('Validation Confusion Matrix:')\n",
    "print(val_confusion_matrix)\n",
    "print('Validation Classification Report:')\n",
    "print(val_classification_report)\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = model.predict(x_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_confusion_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print('Test Confusion Matrix:')\n",
    "print(test_confusion_matrix)\n",
    "print('Test Classification Report:')\n",
    "print(test_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1747ae41",
   "metadata": {},
   "source": [
    "### Create and train Logistic Regression Model for the scaled data\n",
    "\n",
    "This is the first model with the data that has been preprocessed and scaled, but not transformed for normality. The accuracy was 100%, leading us to believe that the model is overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "257255a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 1.0\n",
      "Validation Confusion Matrix:\n",
      "[[116831      0]\n",
      " [     0  39822]]\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    116831\n",
      "           1       1.00      1.00      1.00     39822\n",
      "\n",
      "    accuracy                           1.00    156653\n",
      "   macro avg       1.00      1.00      1.00    156653\n",
      "weighted avg       1.00      1.00      1.00    156653\n",
      "\n",
      "Test Accuracy: 1.0\n",
      "Test Confusion Matrix:\n",
      "[[116832      0]\n",
      " [     0  39822]]\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    116832\n",
      "           1       1.00      1.00      1.00     39822\n",
      "\n",
      "    accuracy                           1.00    156654\n",
      "   macro avg       1.00      1.00      1.00    156654\n",
      "weighted avg       1.00      1.00      1.00    156654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logreg model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_val_pred = model.predict(x_val_scaled)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_confusion_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n",
    "print('Validation Confusion Matrix:')\n",
    "print(val_confusion_matrix)\n",
    "print('Validation Classification Report:')\n",
    "print(val_classification_report)\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = model.predict(x_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_confusion_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print('Test Confusion Matrix:')\n",
    "print(test_confusion_matrix)\n",
    "print('Test Classification Report:')\n",
    "print(test_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f2c69",
   "metadata": {},
   "source": [
    "### Create and train Logistic Regression Model for yeo-johnson transformed data\n",
    "\n",
    "This is the first model with the data that has been preprocessed and transformed, but not scaled. The accuracy was 100%, leading us to believe that the model is also overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd8ef62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 1.0\n",
      "Validation Confusion Matrix:\n",
      "[[116831      0]\n",
      " [     0  39822]]\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    116831\n",
      "           1       1.00      1.00      1.00     39822\n",
      "\n",
      "    accuracy                           1.00    156653\n",
      "   macro avg       1.00      1.00      1.00    156653\n",
      "weighted avg       1.00      1.00      1.00    156653\n",
      "\n",
      "Test Accuracy: 1.0\n",
      "Test Confusion Matrix:\n",
      "[[116832      0]\n",
      " [     0  39822]]\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    116832\n",
      "           1       1.00      1.00      1.00     39822\n",
      "\n",
      "    accuracy                           1.00    156654\n",
      "   macro avg       1.00      1.00      1.00    156654\n",
      "weighted avg       1.00      1.00      1.00    156654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logreg model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train_transformed, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_val_pred = model.predict(x_val_transformed)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_confusion_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n",
    "print('Validation Confusion Matrix:')\n",
    "print(val_confusion_matrix)\n",
    "print('Validation Classification Report:')\n",
    "print(val_classification_report)\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = model.predict(x_test_transformed)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_confusion_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print('Test Confusion Matrix:')\n",
    "print(test_confusion_matrix)\n",
    "print('Test Classification Report:')\n",
    "print(test_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce41b5",
   "metadata": {},
   "source": [
    "### Create and train Logistic Regression Model for yeo-johnson transformed and scaled data\n",
    "\n",
    "This is the first model with the data that has been preprocessed, scaled, and transformed for normality. The accuracy was 100%, leading us to believe that the model is also overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a8b7521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 1.0\n",
      "Validation Confusion Matrix:\n",
      "[[116831      0]\n",
      " [     0  39822]]\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    116831\n",
      "           1       1.00      1.00      1.00     39822\n",
      "\n",
      "    accuracy                           1.00    156653\n",
      "   macro avg       1.00      1.00      1.00    156653\n",
      "weighted avg       1.00      1.00      1.00    156653\n",
      "\n",
      "Test Accuracy: 1.0\n",
      "Test Confusion Matrix:\n",
      "[[116832      0]\n",
      " [     0  39822]]\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    116832\n",
      "           1       1.00      1.00      1.00     39822\n",
      "\n",
      "    accuracy                           1.00    156654\n",
      "   macro avg       1.00      1.00      1.00    156654\n",
      "weighted avg       1.00      1.00      1.00    156654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logreg model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train_trans_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_val_pred = model.predict(x_val_trans_scaled)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_confusion_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n",
    "print('Validation Confusion Matrix:')\n",
    "print(val_confusion_matrix)\n",
    "print('Validation Classification Report:')\n",
    "print(val_classification_report)\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = model.predict(x_test_trans_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_confusion_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print('Test Confusion Matrix:')\n",
    "print(test_confusion_matrix)\n",
    "print('Test Classification Report:')\n",
    "print(test_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52b7ac",
   "metadata": {},
   "source": [
    "### Create and train Logistic Regression Model for the PCA transformed data (orig)\n",
    "\n",
    "This is the fifth model with the data that has been preprocessed, but not scaled nor transformed for normality. The accuracy was about 81%, which is the best model so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ebc563c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8902414891511813\n",
      "Validation Confusion Matrix:\n",
      "[[114461   2370]\n",
      " [ 14824  24998]]\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93    116831\n",
      "           1       0.91      0.63      0.74     39822\n",
      "\n",
      "    accuracy                           0.89    156653\n",
      "   macro avg       0.90      0.80      0.84    156653\n",
      "weighted avg       0.89      0.89      0.88    156653\n",
      "\n",
      "Test Accuracy: 0.8902166558147254\n",
      "Test Confusion Matrix:\n",
      "[[114458   2374]\n",
      " [ 14824  24998]]\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93    116832\n",
      "           1       0.91      0.63      0.74     39822\n",
      "\n",
      "    accuracy                           0.89    156654\n",
      "   macro avg       0.90      0.80      0.84    156654\n",
      "weighted avg       0.89      0.89      0.88    156654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logreg model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train_pca, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_val_pred = model.predict(x_val_pca)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_confusion_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n",
    "print('Validation Confusion Matrix:')\n",
    "print(val_confusion_matrix)\n",
    "print('Validation Classification Report:')\n",
    "print(val_classification_report)\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = model.predict(x_test_pca)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_confusion_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print('Test Confusion Matrix:')\n",
    "print(test_confusion_matrix)\n",
    "print('Test Classification Report:')\n",
    "print(test_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b0b7d7",
   "metadata": {},
   "source": [
    "### Create and train Logistic Regression Model for the PCA transformed data (scaled)\n",
    "\n",
    "This is the sixth model with the data that has been preprocessed and scaled, but not transformed for normality. The accuracy was about 82%, which is the best model so far beating the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0cfd633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8236739800706019\n",
      "Validation Confusion Matrix:\n",
      "[[107864   8967]\n",
      " [ 18655  21167]]\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.89    116831\n",
      "           1       0.70      0.53      0.61     39822\n",
      "\n",
      "    accuracy                           0.82    156653\n",
      "   macro avg       0.78      0.73      0.75    156653\n",
      "weighted avg       0.81      0.82      0.81    156653\n",
      "\n",
      "Test Accuracy: 0.82516245994357\n",
      "Test Confusion Matrix:\n",
      "[[108066   8766]\n",
      " [ 18623  21199]]\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.89    116832\n",
      "           1       0.71      0.53      0.61     39822\n",
      "\n",
      "    accuracy                           0.83    156654\n",
      "   macro avg       0.78      0.73      0.75    156654\n",
      "weighted avg       0.82      0.83      0.82    156654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logreg model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train_scaled_pca, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_val_pred = model.predict(x_val_scaled_pca)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_confusion_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "val_classification_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n",
    "print('Validation Confusion Matrix:')\n",
    "print(val_confusion_matrix)\n",
    "print('Validation Classification Report:')\n",
    "print(val_classification_report)\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = model.predict(x_test_scaled_pca)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_confusion_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print('Test Confusion Matrix:')\n",
    "print(test_confusion_matrix)\n",
    "print('Test Classification Report:')\n",
    "print(test_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ab45a-27f0-4cad-ab0e-45d49b5988f3",
   "metadata": {},
   "source": [
    "We opted to use PCA-transformed and scaled data for creating and training our Logistic Regression model for several compelling reasons:\n",
    "\n",
    "1. **Dimensionality Reduction**:\n",
    "   - **Principal Component Analysis (PCA)** is a powerful technique used to reduce the dimensionality of our dataset while retaining the most important information. This helps in eliminating redundant and less informative features, leading to a more efficient and interpretable model.\n",
    "\n",
    "2. **Feature Scaling**:\n",
    "   - Scaling our data ensures that all features contribute equally to the model. Logistic Regression, like many machine learning algorithms, performs better when the data is normalized, preventing features with larger scales from dominating the model training process.\n",
    "\n",
    "3. **Model Performance**:\n",
    "   - The Logistic Regression model trained on PCA-transformed and scaled data achieved an accuracy of about 82%. This is a significant improvement over previous models and is currently our best-performing model. The use of PCA likely helped in capturing the underlying structure of the data more effectively.\n",
    "\n",
    "4. **Overfitting Reduction**:\n",
    "   - By reducing the number of features, PCA helps in minimizing the risk of overfitting. Overfitting occurs when the model is too complex and captures noise in the data, rather than the actual underlying pattern. PCA helps in addressing this by simplifying the feature set.\n",
    "\n",
    "5. **Computational Efficiency**:\n",
    "   - With fewer features after PCA, the computational cost of training the Logistic Regression model decreases. This makes the model training process faster and more resource-efficient, which is particularly beneficial when dealing with large datasets.\n",
    "\n",
    "Using PCA-transformed and scaled data has led to a significant improvement in model accuracy and overall performance, justifying our decision to incorporate these preprocessing steps in our modeling pipeline. The 82% accuracy stands as evidence to the effectiveness of this approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f37cc0-d134-4b3e-882e-59636e0d241d",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47db47b6-5b86-4739-be26-fe531723ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df to NumPy array\n",
    "x_train_np = x_train_scaled_pca.to_numpy()\n",
    "x_val_np = x_val_scaled_pca.to_numpy()\n",
    "x_test_np = x_test_scaled_pca.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "177cbadf-7cfe-4403-8b4a-4971152fb05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a fraction of the data\n",
    "\n",
    "# define the sample size\n",
    "sample_size = 100000\n",
    "\n",
    "# denerate random indices for sampling\n",
    "np.random.seed(42)\n",
    "\n",
    "# sample the training data\n",
    "sample_indices_train = np.random.choice(x_train_np.shape[0], size=sample_size, replace=False)\n",
    "x_train_sampled = x_train_np[sample_indices_train]\n",
    "y_train_sampled = y_train[sample_indices_train]\n",
    "\n",
    "# sample the validation data independently\n",
    "sample_indices_val = np.random.choice(x_val_np.shape[0], size=sample_size, replace=False)\n",
    "x_val_sampled = x_val_np[sample_indices_val]\n",
    "y_val_sampled = y_val[sample_indices_val]\n",
    "\n",
    "# sample the test data independently\n",
    "sample_indices_test = np.random.choice(x_test_np.shape[0], size=sample_size, replace=False)\n",
    "x_test_sampled = x_test_np[sample_indices_test]\n",
    "y_test_sampled = y_test[sample_indices_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12aa7239-faa3-41b6-bf93-1b817e4b51a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data for RNN: (samples, time_steps, features)\n",
    "# we assume height and width as 1, and channels as 3 (principal components)\n",
    "x_train_rnn = x_train_sampled.reshape((x_train_sampled.shape[0], 1, x_train_sampled.shape[1]))\n",
    "x_val_rnn = x_val_sampled.reshape((x_val_sampled.shape[0], 1, x_val_sampled.shape[1]))\n",
    "x_test_rnn = x_test_sampled.reshape((x_test_sampled.shape[0], 1, x_test_sampled.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c275ec3-6451-4d45-8dfe-72d0fe480a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, SimpleRNN, Dense\n",
    "#define RNN modelfunction\n",
    "def build_rnn_model(hp):\n",
    "    model = Sequential([\n",
    "        Input(shape=(x_train_rnn.shape[1], x_train_rnn.shape[2])),\n",
    "        SimpleRNN(units=hp.Int('rnn_units', min_value=50, max_value=100, step=50), activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb342af8-f972-4f22-b6bb-b87ee5c90a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Reloading Tuner from models/rnn/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# initalize and run Keras Turner\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    tuner_rnn = kt.Hyperband(build_rnn_model,\n",
    "                             objective='val_accuracy',\n",
    "                             max_epochs=5,\n",
    "                             factor=3,\n",
    "                             directory='models',\n",
    "                             project_name='rnn')\n",
    "\n",
    "# define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ceaeec0-6afc-42a5-ac6a-d01415210a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparemeter tuning with earlt stopping \n",
    "tuner_rnn.search(x_train_rnn, y_train_sampled,\n",
    "                 epochs=5,  \n",
    "                 validation_data=(x_val_rnn, y_val_sampled),\n",
    "                 callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fa1ea4d-6390-4a9e-8509-07061a8a8f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'rnn_units': 50, 'tuner/epochs': 2, 'tuner/initial_epoch': 0, 'tuner/bracket': 1, 'tuner/round': 0}\n"
     ]
    }
   ],
   "source": [
    "# compile best model - post hyperparemeter tuning \n",
    "best_hps_rnn = tuner_rnn.get_best_hyperparameters()[0]\n",
    "print(f'Best Hyperparameters: {best_hps_rnn.values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48f78714-8d71-4fab-be9e-3d2082b788c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "rnn_model = tuner_rnn.hypermodel.build(best_hps_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cbc64f-beac-4183-aff2-1c07c5926b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "history_rnn = rnn_model.fit(x_train_rnn, y_train_sampled,\n",
    "                            epochs=10, batch_size=16,\n",
    "                            validation_data=(x_val_rnn, y_val_sampled),\n",
    "                            callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b03096-b4d5-4650-b15d-1b02de5d6f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test data \n",
    "y_test_pred = rnn_model.predict(x_test_rnn)\n",
    "test_predictions = (y_test_pred > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test_sampled, test_predictions)\n",
    "test_confusion_matrix = confusion_matrix(y_test_sampled, test_predictions)\n",
    "test_classification_report = classification_report(y_test_sampled, test_predictions)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print('Test Confusion Matrix:')\n",
    "print(test_confusion_matrix)\n",
    "print('Test Classification Report:')\n",
    "print(test_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7215ac1f-f620-436b-b873-6ae9ccfce7f9",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c4a42-8041-48e5-9629-d7f7f41e682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data for CNN: (samples, height, width, channels)\n",
    "# we assume height and width as 1, and channels as 3 (principal components)\n",
    "x_train_cnn = x_train_sampled.reshape((x_train_sampled.shape[0], 1, 3))\n",
    "x_val_cnn = x_val_sampled.reshape((x_val_sampled.shape[0], 1, 3))\n",
    "x_test_cnn = x_test_sampled.reshape((x_test_sampled.shape[0], 1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c68fc-6e7b-44a5-810a-6bb2248dd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model-building function\n",
    "def build_cnn_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=hp.Int('filters', min_value=32, max_value=128, step=32),\n",
    "                     kernel_size=hp.Choice('kernel_size', values=[1, 2, 3]),\n",
    "                     activation='relu',\n",
    "                     input_shape=(1, 3)))  \n",
    "    model.add(MaxPooling1D(pool_size=1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=hp.Choice('optimizer', ['adam', 'sgd']),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2fe99-d72e-4d6a-a922-e66f344c3c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and run KerasTuner\n",
    "tuner_cnn = kt.Hyperband(build_cnn_model,\n",
    "                         objective='val_accuracy',\n",
    "                         max_epochs=5,\n",
    "                         factor=3,\n",
    "                         directory='models',\n",
    "                         project_name='cnn')\n",
    "\n",
    "tuner_cnn.search(x_train_cnn, y_train_sampled,\n",
    "                 epochs=10,\n",
    "                 validation_data=(x_val_cnn, y_val_sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f46181-8cd1-401f-9f87-fe7edeeaee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile best model - post hyperparemeter tuning \n",
    "best_hps = tuner_cnn.get_best_hyperparameters()[0]\n",
    "print(f'Best Hyperparameters: {best_hps.values}')\n",
    "\n",
    "# train model\n",
    "cnn_model = tuner_cnn.hypermodel.build(best_hps)\n",
    "history = cnn_model.fit(x_train_cnn, y_train_sampled,\n",
    "                        epochs=10, batch_size=32,\n",
    "                        validation_data=(x_val_cnn, y_val_sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a6df0-e8e3-4036-aaca-5c69ec76fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set \n",
    "y_test_pred = cnn_model.predict(x_test_cnn)\n",
    "test_predictions = (y_test_pred > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test_sampled, test_predictions)\n",
    "test_confusion_matrix = confusion_matrix(y_test_sampled, test_predictions)\n",
    "test_classification_report = classification_report(y_test_sampled, test_predictions)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print('Test Confusion Matrix:')\n",
    "print(test_confusion_matrix)\n",
    "print('Test Classification Report:')\n",
    "print(test_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af25050-d239-47c3-85d9-119ddfcaf564",
   "metadata": {},
   "source": [
    "## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c941e2aa-7a09-48e1-a072-f1ee0d38e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# define parameter grid for DBSCAN\n",
    "param_grid_dbscan = {\n",
    "    'eps': [0.3, 0.5, 0.7],\n",
    "    'min_samples': [5, 10, 15]\n",
    "}\n",
    "\n",
    "best_score = -1\n",
    "best_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19566dd5-d097-4326-b012-e542afd7a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dierectory to save best parameters\n",
    "directory = 'models/dbscan'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "best_params_file = os.path.join(directory, 'best_params.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def85f11-4e66-4f82-9d3e-be6354e951d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_params(params):\n",
    "    try:\n",
    "        model_dbscan = DBSCAN(eps=params['eps'], min_samples=params['min_samples'])\n",
    "        y_train_pred = model_dbscan.fit_predict(x_train_np)\n",
    "\n",
    "        if len(set(y_train_pred)) > 1 and -1 not in set(y_train_pred):\n",
    "            score = silhouette_score(x_train_np, y_train_pred)\n",
    "            return (params, score)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with parameters {params}: {e}\")\n",
    "    return (params, -1)\n",
    "\n",
    "results = Parallel(n_jobs=-1)(delayed(evaluate_params)(params) for params in ParameterGrid(param_grid_dbscan))\n",
    "\n",
    "for params, score in results:\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254040b6-c4e2-44f2-984b-ef1ba98fb016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best parameters and score\n",
    "with open(best_params_file, 'w') as f:\n",
    "    json.dump({'best_params': best_params, 'best_score': best_score}, f)\n",
    "\n",
    "print(f'Best Parameters for DBSCAN: {best_params}')\n",
    "print(f'Best Silhouette Score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79dea3b-a628-4f8e-b832-8dece8976115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the best parameters and score\n",
    "def load_best_params(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data['best_params'], data['best_score']\n",
    "\n",
    "# loading the best parameters\n",
    "loaded_params, loaded_score = load_best_params(best_params_file)\n",
    "print(f'Loaded Best Parameters: {loaded_params}')\n",
    "print(f'Loaded Best Silhouette Score: {loaded_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d01638-33e6-4da5-ae9c-9ed0fe470128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best parameters to fit DBSCAN on the full training data\n",
    "dbscan_model = DBSCAN(eps=loaded_params['eps'], min_samples=loaded_params['min_samples'])\n",
    "y_train_best_pred = dbscan_model.fit_predict(x_train_np)\n",
    "\n",
    "labels_file = os.path.join(directory, 'dbscan_labels.npy')\n",
    "np.save(labels_file, y_train_best_pred)\n",
    "\n",
    "loaded_labels = np.load(labels_file)\n",
    "print(f'Loaded DBSCAN Labels: {loaded_labels}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
